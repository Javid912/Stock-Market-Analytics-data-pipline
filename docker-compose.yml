# DataPipe Analytics Docker Compose Configuration
# This file defines our multi-container application architecture with proper build and runtime dependencies.
#
# Service Dependency Chain:
# postgres → test → airflow-init → airflow-webserver/scheduler → dbt → dashboard
#
# Runtime Dependencies:
# - Defined in 'depends_on' with conditions
# - Controls startup order and health checks
#
# Health Checks:
# - service_healthy: Waits for health check to pass
# - service_completed_successfully: Waits for one-time tasks to complete
# - service_started: Just waits for container to start

version: '3.8'

services:
  # Base Database Service
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    ports:
      - "5432:5432"  # Host:Container port mapping
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/migrations:/docker-entrypoint-initdb.d
      - ./sql:/sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
    networks:
      - datapipe_network
    restart: unless-stopped

  # Testing Service
  # Runs unit tests against the codebase
  test:
    build:
      context: .  # Build context is root directory
      dockerfile: docker/test.Dockerfile
    volumes:
      - .:/app  # Mount entire project for testing
    env_file:
      - .env
    command: pytest tests/unit/ -v
    networks:
      - datapipe_network
    depends_on:
      postgres:
        condition: service_healthy  # Waits for postgres health check

  # Airflow Initialization Service
  # One-time setup of Airflow database and admin user
  airflow-init:
    build:
      context: .
      dockerfile: docker/airflow.Dockerfile
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.com'
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - datapipe_network

  # Airflow Web Interface
  # Provides UI for DAG management
  airflow-webserver:
    build:
      context: .
      dockerfile: docker/airflow.Dockerfile
    command: airflow webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}
      - AIRFLOW__WEBSERVER__WORKERS=2
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
    volumes:
      - ./airflow/dags:/opt/airflow/dags  # DAG files
      - ./airflow/plugins:/opt/airflow/plugins  # Airflow plugins
      - ./src:/opt/airflow/src  # Source code
      - ./dbt:/dbt  # DBT project files
      - ./tests:/opt/airflow/tests  # Add this line
      - airflow_logs:/opt/airflow/logs  # Persistent log storage
    ports:
      - "8080:8080"  # Airflow UI port
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - datapipe_network
    restart: unless-stopped

  # Airflow Scheduler
  # Executes DAGs and tasks
  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/airflow.Dockerfile
    command: airflow scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}
      - AIRFLOW__SCHEDULER__MAX_THREADS=2
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./dbt:/dbt
      - ./tests:/opt/airflow/tests
      - airflow_logs:/opt/airflow/logs
    networks:
      - datapipe_network
    restart: unless-stopped

  # DBT Transformation Service
  # Handles data transformations
  dbt:
    build:
      context: .
      dockerfile: docker/dbt.Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    volumes:
      - ./dbt:/dbt  # DBT project files
    environment:
      - DBT_PROFILES_DIR=/dbt
    user: root
    networks:
      - datapipe_network
    command: tail -f /dev/null  # Keep container running

  # Dashboard Service
  # Streamlit web application for data visualization
  dashboard:
    build:
      context: ./src/dashboard
      dockerfile: Dockerfile
    ports:
      - "8501:8501"  # Dashboard UI port
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    depends_on:
      postgres:
        condition: service_healthy
      dbt:
        condition: service_started  # Just wait for the container to start
    networks:
      - datapipe_network
    volumes:
      - ./src/dashboard:/app  # Dashboard application code
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8501"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Metabase Service
  # Business intelligence and analytics platform
  metabase:
    image: metabase/metabase:latest
    ports:
      - "3000:3000"
    environment:
      - MB_DB_TYPE=postgres
      - MB_DB_DBNAME=postgres
      - MB_DB_PORT=5432
      - MB_DB_USER=postgres
      - MB_DB_PASS=postgres
      - MB_DB_HOST=postgres
      - MB_ENCRYPTION_SECRET_KEY=your_secret_key_here
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - datapipe_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

# Network Configuration
# All services communicate through this internal network
networks:
  datapipe_network:
    name: datapipe_network

# Persistent Storage Volumes
volumes:
  postgres_data:  # Database files
    name: datapipe_postgres_data
  postgres_logs:  # Database logs
    name: datapipe_postgres_logs
  airflow_logs:  # Airflow logs
    name: datapipe_airflow_logs
  dbt_logs:  # DBT logs
    name: datapipe_dbt_logs